{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim-shibanov/anaconda3/envs/vllm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:02<00:00,  7.08it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum entanglement like I'm five.\n",
      "\n",
      "Quantum entanglement is a special connection between two or more particles. Imagine you have two magic dice. No matter how far apart you move them, when you roll them, they always show the same number. If one shows a \n"
     ]
    }
   ],
   "source": [
    "# ✅ Prepare input\n",
    "input_text = \"Explain quantum entanglement like I'm five.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# ✅ Send input to CPU (where model is)\n",
    "inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "\n",
    "# ✅ Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# ✅ Print output\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
